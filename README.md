# ET-BERT

[![codebeat badge](https://codebeat.co/badges/f75fab90-6d00-44b4-bb42-d19067400243)](https://codebeat.co/projects/github-com-linwhitehat-et-bert-main)
![](https://img.shields.io/badge/license-MIT-000000.svg)
[![arXiv](https://img.shields.io/badge/arXiv-1909.05658-<color>.svg)](https://arxiv.org/abs/1909.05658)

<br/>

Table of Contents
=================
  * [Introduction](#introduction)
  * [Requirements](#requirements)
  * [Datasets](#datasets)
  * [Download ET-BERT](#download)
  * [Using ET-BERT](#using)
  * [Reproduce ET-BERT](#reproduce)
  * [Contact information](#contact-information)
  * [Citation](#citation)

<br/>

## Citation
#### If you are using the work (e.g. pre-trained model) in ET-BERT for academic work, please cite the [paper](https://arxiv.org/pdf/1909.05658.pdf) published in WWW 2022:

Xinjie Lin, Gang Xiong, Gaopeng Gou, Zhen Li, Junzheng Shi and Jing Yu. 2022. ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification. In Proceedings of The Web Conference (WWW), Lyon, France. ACM, New York, NY, USA. 

```
@inproceedings{Lin2022etbert,
  author        = {Lin, Xinjie and Xiong, Gang and Gou, Gaopeng and Li, Zhen and Shi, Junzheng and Yu, Jing.},
  title         = {ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification},
  year          = {2022},
  month         = apr,
  isbn          = {9781450390965},
  publisher     = {Association for Computing Machinery},
  booktitle     = {Proceedings of The Web Conference 2022},
  numpages      = {10},
}
```

## Contact information
Please post a Github issue if you have any questions.
